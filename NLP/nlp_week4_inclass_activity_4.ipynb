{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05d232a3-c4c6-4fed-80c8-32b2618abaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rajat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rajat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rajat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51b3013b-fabb-4a21-adb2-9ea11ff926d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = np.array(['Feature Extraction aims to reduce the number of features in a dataset by creating new features from the existing ones (and then discarding the original features). These new reduced set of features should then be able to summarize most of the information contained in the original set of features!!!. In this way, a summarised version of the original features can be created from a combination of the original set!!!',\n",
    "                      'Another commonly used technique to reduce the number of feature in a dataset is Feature Selection! The difference between Feature Selection and/or Feature Extraction is that feature selection aims instead to $ rank the importance of the existing features in the dataset and discard less important ones (no new features are created)?!. If you are interested in finding out more about Feature Selection, you can find more information about it in my previous article.',\n",
    "                      'In this article, I will walk you through how to apply Feature Extraction techniques using the Kaggle Mushroom Classification Dataset as an example??? Our objective will be to try to predict if a Mushroom is poisonous or not by looking at the given features. All the code used in this post (and more!) is available on Kaggle and on my GitHub Account.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b70aa773-9411-4265-8ab0-283a8ad0e9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    words = word_tokenize(text)          # Tokenize the text into words\n",
    "    filtered_words = [word.lower() for word in words if word.lower() not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "processed_text_data_step1 = [preprocess_text(text) for text in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7920661-230f-49a7-b167-fbc5622b1690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stemming and lemmatization\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def apply_stemming_and_lemmatization(text):\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "processed_text_data_step2 = [apply_stemming_and_lemmatization(text) for text in processed_text_data_step1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc357e1b-ae9a-48b1-9b6b-fe24e863e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part-of-Speech Tagging\n",
    "def pos_tagging(text):\n",
    "    words = word_tokenize(text)\n",
    "    tagged_words = pos_tag(words)\n",
    "    return tagged_words\n",
    "\n",
    "processed_text_data_step3 = [pos_tagging(text) for text in processed_text_data_step2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf175818-d105-48d6-ad35-3f8318ff1f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "vectorizer = CountVectorizer()\n",
    "bag_of_words_matrix = vectorizer.fit_transform(processed_text_data_step2)\n",
    "bag_of_words_feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1d1af8c-a2b5-4b36-b121-ad6cb63b788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_text_data_step2)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bc59898-cc4c-4a48-bf0a-a0121efe6566",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove stopwords and punctuation\n",
      "['feature extraction aims reduce number features dataset creating new features existing ones discarding original features new reduced set features able summarize information contained original set features way summarised version original features created combination original set', 'another commonly used technique reduce number feature dataset feature selection difference feature selection andor feature extraction feature selection aims instead rank importance existing features dataset discard less important ones new features created interested finding feature selection find information previous article', 'article walk apply feature extraction techniques using kaggle mushroom classification dataset example objective try predict mushroom poisonous looking given features code used post available kaggle github account']\n",
      "Apply stemming and lemmatization\n",
      "['featur extract aim reduc number featur dataset creat new featur exist one discard origin featur new reduc set featur abl summar inform contain origin set featur way summaris version origin featur creat combin origin set', 'anoth commonli use techniqu reduc number featur dataset featur select differ featur select andor featur extract featur select aim instead rank import exist featur dataset discard le import one new featur creat interest find featur select find inform previou articl', 'articl walk appli featur extract techniqu use kaggl mushroom classif dataset exampl object tri predict mushroom poison look given featur code use post avail kaggl github account']\n",
      "Part-of-Speech Tagging\n",
      "[[('featur', 'JJ'), ('extract', 'NN'), ('aim', 'NN'), ('reduc', 'NN'), ('number', 'NN'), ('featur', 'JJ'), ('dataset', 'NN'), ('creat', 'NN'), ('new', 'JJ'), ('featur', 'JJ'), ('exist', 'VBP'), ('one', 'CD'), ('discard', 'NN'), ('origin', 'NN'), ('featur', 'VBD'), ('new', 'JJ'), ('reduc', 'NN'), ('set', 'VBN'), ('featur', 'JJ'), ('abl', 'JJ'), ('summar', 'NN'), ('inform', 'NN'), ('contain', 'NN'), ('origin', 'NN'), ('set', 'VBN'), ('featur', 'JJ'), ('way', 'NN'), ('summaris', 'JJ'), ('version', 'NN'), ('origin', 'NN'), ('featur', 'JJ'), ('creat', 'NN'), ('combin', 'NN'), ('origin', 'NN'), ('set', 'NN')], [('anoth', 'DT'), ('commonli', 'NN'), ('use', 'NN'), ('techniqu', 'NN'), ('reduc', 'NN'), ('number', 'NN'), ('featur', 'JJ'), ('dataset', 'NN'), ('featur', 'NN'), ('select', 'NN'), ('differ', 'NN'), ('featur', 'NN'), ('select', 'JJ'), ('andor', 'NN'), ('featur', 'NN'), ('extract', 'NN'), ('featur', 'NN'), ('select', 'JJ'), ('aim', 'NN'), ('instead', 'RB'), ('rank', 'VBD'), ('import', 'JJ'), ('exist', 'NN'), ('featur', 'NN'), ('dataset', 'VB'), ('discard', 'NN'), ('le', 'NN'), ('import', 'NN'), ('one', 'CD'), ('new', 'JJ'), ('featur', 'NN'), ('creat', 'NN'), ('interest', 'NN'), ('find', 'VBP'), ('featur', 'JJ'), ('select', 'JJ'), ('find', 'NN'), ('inform', 'NN'), ('previou', 'NN'), ('articl', 'NN')], [('articl', 'NN'), ('walk', 'NN'), ('appli', 'IN'), ('featur', 'JJ'), ('extract', 'NN'), ('techniqu', 'NN'), ('use', 'NN'), ('kaggl', 'NN'), ('mushroom', 'NN'), ('classif', 'NN'), ('dataset', 'NN'), ('exampl', 'NN'), ('object', 'VBP'), ('tri', 'NN'), ('predict', 'NN'), ('mushroom', 'NN'), ('poison', 'NN'), ('look', 'VB'), ('given', 'VBN'), ('featur', 'JJ'), ('code', 'NN'), ('use', 'NN'), ('post', 'NN'), ('avail', 'NN'), ('kaggl', 'NN'), ('github', 'NN'), ('account', 'NN')]]\n",
      "Bag of Words\n",
      "[[1 0 1 0 0 0 0 0 0 0 1 0 1 2 1 0 1 0 1 1 7 0 0 0 0 1 0 0 0 0 0 0 2 1 0 1\n",
      "  4 0 0 0 0 0 2 0 3 1 1 0 0 0 1 0 1]\n",
      " [0 0 1 1 1 0 1 0 0 0 0 1 0 1 2 1 1 0 1 1 8 2 0 0 2 1 1 1 0 1 0 0 1 1 0 1\n",
      "  0 0 0 0 1 1 1 4 0 0 0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1 2 0 1 1 0 0 0 0 2 0 1 2 0 0 1 0\n",
      "  0 1 1 1 0 0 0 0 0 0 0 1 1 2 0 1 0]]\n",
      "Feature Names: ['abl' 'account' 'aim' 'andor' 'anoth' 'appli' 'articl' 'avail' 'classif'\n",
      " 'code' 'combin' 'commonli' 'contain' 'creat' 'dataset' 'differ' 'discard'\n",
      " 'exampl' 'exist' 'extract' 'featur' 'find' 'github' 'given' 'import'\n",
      " 'inform' 'instead' 'interest' 'kaggl' 'le' 'look' 'mushroom' 'new'\n",
      " 'number' 'object' 'one' 'origin' 'poison' 'post' 'predict' 'previou'\n",
      " 'rank' 'reduc' 'select' 'set' 'summar' 'summaris' 'techniqu' 'tri' 'use'\n",
      " 'version' 'walk' 'way']\n",
      "TF-IDF\n",
      "[[0.12888333 0.         0.0980191  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.12888333 0.\n",
      "  0.12888333 0.19603819 0.07612057 0.         0.0980191  0.\n",
      "  0.0980191  0.07612057 0.532844   0.         0.         0.\n",
      "  0.         0.0980191  0.         0.         0.         0.\n",
      "  0.         0.         0.19603819 0.0980191  0.         0.0980191\n",
      "  0.51553332 0.         0.         0.         0.         0.\n",
      "  0.19603819 0.         0.38664999 0.12888333 0.12888333 0.\n",
      "  0.         0.         0.12888333 0.         0.12888333]\n",
      " [0.         0.         0.09505838 0.12499035 0.12499035 0.\n",
      "  0.09505838 0.         0.         0.         0.         0.12499035\n",
      "  0.         0.09505838 0.14764263 0.12499035 0.09505838 0.\n",
      "  0.09505838 0.07382131 0.59057051 0.2499807  0.         0.\n",
      "  0.2499807  0.09505838 0.12499035 0.12499035 0.         0.12499035\n",
      "  0.         0.         0.09505838 0.09505838 0.         0.09505838\n",
      "  0.         0.         0.         0.         0.12499035 0.12499035\n",
      "  0.09505838 0.4999614  0.         0.         0.         0.09505838\n",
      "  0.         0.09505838 0.         0.         0.        ]\n",
      " [0.         0.18710929 0.         0.         0.         0.18710929\n",
      "  0.14230144 0.18710929 0.18710929 0.18710929 0.         0.\n",
      "  0.         0.         0.11050976 0.         0.         0.18710929\n",
      "  0.         0.11050976 0.22101952 0.         0.18710929 0.18710929\n",
      "  0.         0.         0.         0.         0.37421858 0.\n",
      "  0.18710929 0.37421858 0.         0.         0.18710929 0.\n",
      "  0.         0.18710929 0.18710929 0.18710929 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.14230144\n",
      "  0.18710929 0.28460287 0.         0.18710929 0.        ]]\n",
      "Feature Names: ['abl' 'account' 'aim' 'andor' 'anoth' 'appli' 'articl' 'avail' 'classif'\n",
      " 'code' 'combin' 'commonli' 'contain' 'creat' 'dataset' 'differ' 'discard'\n",
      " 'exampl' 'exist' 'extract' 'featur' 'find' 'github' 'given' 'import'\n",
      " 'inform' 'instead' 'interest' 'kaggl' 'le' 'look' 'mushroom' 'new'\n",
      " 'number' 'object' 'one' 'origin' 'poison' 'post' 'predict' 'previou'\n",
      " 'rank' 'reduc' 'select' 'set' 'summar' 'summaris' 'techniqu' 'tri' 'use'\n",
      " 'version' 'walk' 'way']\n"
     ]
    }
   ],
   "source": [
    "print(\"Remove stopwords and punctuation\")\n",
    "print(processed_text_data_step1)\n",
    "print(\"Apply stemming and lemmatization\")\n",
    "print(processed_text_data_step2)\n",
    "print(\"Part-of-Speech Tagging\")\n",
    "print(processed_text_data_step3)\n",
    "print(\"Bag of Words\")\n",
    "print(bag_of_words_matrix.toarray())\n",
    "print(\"Feature Names:\", bag_of_words_feature_names)\n",
    "print(\"TF-IDF\")\n",
    "print(tfidf_matrix.toarray())\n",
    "print(\"Feature Names:\", tfidf_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce3481c-7c16-4137-a1eb-110c3f5fb168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
